# MalGAN Attack

We implement MalGAN to attack several robust models and evaluate the performance of those robust models.


## Introduction

The GAN attacker has a generator and a substitute detector(discriminator). The generator takes a malware feature vector m, and a noise vector z (append at the end) as input and generates an adversarial example based on that malware sample. The substitute detector is used to fit the behavior of the blackbox classifier and feed the gradient information back to the generator. The idea is based on [Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN](https://arxiv.org/pdf/1702.05983.pdf) by Weiwei Hu and Ying Tan.


## Data

Train Data: 13190 samples, each with 3514 API features. The first 6896 are malware example; the last 6294 are benign examples.

Test Data: 3435 malware samples, each with 3514 API features.


## Models
Baseline<br />
Adv Retrain A<br />
Adv Retrain B<br />
Retrain C<br />
Adv Retrain D<br />
Adv Retrain A+B<br />
Ensemble A+B Base Learner<br />
Ensemble D Base Learner<br />
Robust A   Subtree Deletion Distance 1<br />
Robust B   Subtree Insertion Distance 1<br />
Robust C   Subtree Deletion Distance 2<br />
Robust D   Subtree Insertion Distance 41<br />
Robust E   <br />
Robust A+B  Both subtree deletion and insertion with distance 1<br />
Robust A+B+E <br />


## Setup and Mean of Measurement

### Generator
input: malware + noise (append at the end)<br />
output: adversarial malware examples

### Robust Model
input: adversarial malware examples & benign examples<br />
output: predicted labels for adversarial malware examples & benign examples

### Substitute Detector
input: adversarial malware examples & benign examples and their predicted labels by Robust Model<br />
output: validity

After each epoch, we evaluate the performance of generator on our test data.

Successful Evasion:
A malware has a label of 1 but is wrongly classified by the classifier as a label of 0 (benign)

For each successful evasion malware, we remove it from the test dataset. In other words, we only keep testing the malware that still could not fool the classifier.

For each successful evasion, we store the malware into a dictionary, with key being SHA-1 hash and value being the L0 distance between a malware and its adversarial counterpart.

![Image description](images/Architecture.png)


## Result

After 50 epochs, we plot the graph showing the relationship between model ERA(estimated robust accuracy) and L0 distance between a malware and its adversarial counterpart.
![Image description](images/ERA_result.jpg)

According to the images, robust D, robust E, and robust A+B+E model maintain very high ERA against the MalGAN attack. Baseline, robust A and robust C are fully evaded. Robust B and Robust A+B are not fully evaded.

## Other Result

![Image description](images/robust-baseline-1.png)
![Image description](images/robust-baseline-2.png)

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License
[MIT](https://choosealicense.com/licenses/mit/)
